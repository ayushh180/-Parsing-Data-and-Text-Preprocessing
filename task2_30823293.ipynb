{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assignment 1\n",
    "\n",
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Student name: $ \\text {Ayush Sharma} $\n",
    "\n",
    " Due date: 13 Sept 2020\n",
    "\n",
    "Environment: Python 3.6.0 and Anaconda 4.3.0 (64-bit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Steps followed:\n",
    "1.) Import xlsx data into dataframe and Remove empty rows/columns and Remove non-english tweets using langid Combine all tweets for a sheet.\n",
    "\n",
    "2.) Tokenize combined tweet using RegexpTokenizer\n",
    "\n",
    "3.)Generate top 100 bigrams for each sheet using BigramCollocationFinder and Write top 100 bigrams, count for each date to bigram.txt\n",
    "\n",
    "4.) Remove stop words using stopword file, Remove words appearing in more than 60 sheets and less than 5 sheets, Remove words with length less than 3\n",
    "\n",
    "5.) Stemming all the tokens\n",
    "\n",
    "6.) Get top 100 unigrams for each sheet and count and Write top 100 unigrams, count for each date to unigram.txt\n",
    "\n",
    "7.) Generate top 200 bigrams on complete text from the original tokens\n",
    "\n",
    "8.) Get vocab by taking unique words from all stemmed unigrams Add 200 bigrams to the vocab appending them with underscore\n",
    "\tWrite vocab to vocab.txt\n",
    "    \n",
    "9.) Create sparse matrix by with vocab ids of words in for each date and their count and Write this to countVec.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Used:\n",
    "- re: This Library is used to tokenize the \n",
    "- langid: “langid” package, only keeps the tweets that are in English language\n",
    "- pandas as pd: Load, explore, work on the dataframe that is loaded \n",
    "- nltk: NLTK is a Natural language text processing liabrary used to perform text analysis\n",
    "- nltk.tokenize , RegexpTokenizer: This library tokenise the regular expression passed into It.\n",
    "- from nltk.collocations import * used to create the bigrams\n",
    "- from nltk.stem import PorterStemmer: This library is used form stemming\n",
    "- from itertools import chain: Helps in combing various Lists\n",
    "- from nltk import bigrams: Generate bigram\n",
    "- from nltk import FreqDist: gives frequencey of the bigram or unigram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.) Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import langid\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.collocations import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from itertools import chain\n",
    "from nltk import bigrams\n",
    "from nltk import FreqDist\n",
    "import itertools\n",
    "from nltk.collocations import *\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Step 2: Reading and Analysing data\n",
    "The data contains of 80 sheets of excel that have thousands of tweet text and date along with id \n",
    "- Read the data\n",
    "- parsing through the excel files and sheets and removing unwanted NA's and reshaping it\n",
    "- Applying langid on each text if not english the text will not be appended tp the dictionary\n",
    "- finally creating a dictionary that has date as key and text as a list of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Excel data\n",
    "data=pd.ExcelFile('30823293.xlsx')\n",
    "# creating blank dict for sheets\n",
    "tweetDict={}\n",
    "# Iterating through each sheet and re shaping it into a better and more usefull format \n",
    "for i in range(len(data.sheet_names)):\n",
    "    # parsing through each sheet\n",
    "    df=data.parse(i)\n",
    "    # removing na values\n",
    "    df=df.dropna(0, how = 'all')\n",
    "    df=df.dropna(1, how = 'all')\n",
    "    # Seting columns names\n",
    "    df.columns = [\"Text\",\"Id\",\"Date\"]\n",
    "    # resetting indexes\n",
    "    df.index = range(len(df.index))\n",
    "    df=df.drop([0])\n",
    "    df.index = range(len(df.index))\n",
    "    # iterating over the Text column to perform tasks like checking if the type is str\n",
    "    # Dropping all those values that are not required\n",
    "    for j in df['Text']:\n",
    "        if type(j)!=str:\n",
    "            nonstrindex=df[df['Text']==j].index\n",
    "            df.drop(nonstrindex,inplace=True)\n",
    "            df.reset_index(drop = True)\n",
    "    # Using the langid checking each of the Text to remove all non english tweet\n",
    "        if langid.classify(str(j))[0]!='en':\n",
    "            textindex=df[df['Text']==j].index\n",
    "            df.drop(textindex,inplace=True)\n",
    "            df.reset_index(drop = True)\n",
    "    # Creating a Dictionary with date as key and tweets texts as value\n",
    "    tweetDict[data.sheet_names[i]] = '\\n'.join(df.Text.values).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Tokenizing \n",
    "- Tokenization refers to creating each words in the text as token by applying the regex ` [a-zA-Z]+(?:[-'][a-zA-Z]+)?`\n",
    "- Tokeinzing the data of text and applying regex to create the token\n",
    "- applying  tokenization for to the dictionary that has all the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating tokens\n",
    "tokenizer= RegexpTokenizer(\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "unigaram_token={}\n",
    "# Applying token to the dictionary data\n",
    "for i,j in tweetDict.items():\n",
    "    unigaram_token[i]=tokenizer.tokenize(j)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 Generating 100 Bigram:\n",
    "\n",
    "- Using the Counter function to calculate the top 100 bigrams by using the package nltk.bigrams that finds each bigram for the text\n",
    "- Then structuring the bigrams that we have in a way that includes the count of the bigram and formating according to output file\n",
    "- Finally having a structured formate of a dictionary with key as date and bigrams as a list of values with bigrams within a tuple and a tuple that has the count of each bigram. \n",
    "- Writing a text file that have the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the top 100 bigrams words and counting their occurences \n",
    "bigramdict={}\n",
    "for i,j in unigaram_token.items():\n",
    "    bigramword=list(nltk.bigrams(j))\n",
    "    bigramcount=Counter(bigramword).most_common(100)\n",
    "    bigramdict[i]=bigramcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formating the bigrams extracted in the required way to write the file\n",
    "final_bi_file_str=str()\n",
    "# iteratin within the dict to create the text\n",
    "for i,j in bigramdict.items():\n",
    "    final_bi_file_str=final_bi_file_str+str(i)+':'+str(j)+'\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and writing the 100bi.txt\n",
    "bigramtxtfile = open(\"30823293_100bi.txt\", \"w\",encoding='UTF-8')\n",
    "bigramtxtfile.write(str(final_bi_file_str))\n",
    "\n",
    "bigramtxtfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 Removing stopwords\n",
    "\n",
    "- Stop words are the words least significant words that are appearing frequently like a, an , the , is etc \n",
    "- Stop words are removed using the text file provided by the faculty that will help us remove such words.\n",
    "- we will ieterate over our token dictionary by refering to each key which in our case is the date and look for each element within the value and selecting the element of the value to check if it is a stopword or not if it is a stop word then it will be removed from the data.\n",
    "- after that we have calculated the document frequency of the word by using `FreqDist(list(chain.from_iterable)` to find if a word is in more that 60 document or if less than 5 documents after finding them these words are removed from the dictionary\n",
    "- now appending the new dictionary after handling all the removals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the stopword file provided to us by faculty\n",
    "stopword=[]\n",
    "with open('stopwords_en.txt') as f:\n",
    "    stopword=f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering and removing the stopwords\n",
    "tokenised_dict={}\n",
    "for i,j in unigaram_token.items():\n",
    "    tokenised_dict[i]= [x for x in j if x not in stopword]\n",
    "# Filtering the words with len less that 3 characters within the dictionary    \n",
    "for i,j in tokenised_dict.items():\n",
    "    tokenised_dict[i]= [x for x in j if len(x)>3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# romoving less favaroute word i.e words with < 5 count in the document frequency\n",
    "#and words with >60 in frequency and creating new dict\n",
    "list_final={}\n",
    "for i,j in tokenised_dict.items():\n",
    "    list_final = FreqDist(list(chain.from_iterable([set(x) for x in tokenised_dict.values()])))\n",
    "    \n",
    "list_remove=[]\n",
    "for i in list_final.keys():\n",
    "    if list_final.get(i)>60 or list_final.get(i)<5:\n",
    "        list_remove.append(i)\n",
    "        \n",
    "for i in list_remove:        \n",
    "    list_final.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the above results to new dict\n",
    "tokenised_dict2 = {}\n",
    "for i in tokenised_dict:\n",
    "    lst=tokenised_dict.get(i)\n",
    "    refined_list=[]\n",
    "    for each in lst:\n",
    "        if(each  in list_final):\n",
    "            refined_list.append(each)\n",
    "    tokenised_dict2[i]=refined_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 Stemming\n",
    "\n",
    "- We have used PortterStemmer for stemming because we were asked to use it.\n",
    "\n",
    "-  (or 'Porter stemmer') is a process for removing the commoner morphological and inflexional    endings from words in English.\n",
    "- The Porter stemming algorithm removes the commoner morphological and inflexional endings from words in English. like consider, considering, speed, speeding , sleep, sleeping\n",
    "\n",
    "- remove the words including suffix like `ing` or `e` etc thus such words will be converted to similar words after stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming the tokens using porterstemmer and passing it into a dictionary\n",
    "tokenised_dict3 = {}\n",
    "ps = PorterStemmer()\n",
    "for i in tokenised_dict2:\n",
    "    lst=tokenised_dict2.get(i)\n",
    "    refined_list=[]\n",
    "    for each in lst:\n",
    "        refined_list.append(ps.stem(each))\n",
    "    tokenised_dict3[i]=refined_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 Generating top 100 Unigrams:\n",
    "\n",
    "- In this step we have all the umograms after removing unwanted characters and words and steming the tokens\n",
    "- now we will count the freqency of all the token for each date\n",
    "- Finally we will collect and keep the top 100 unigrams that we need to keep\n",
    "- In the final step we will format the top 100 unigram as per the required output and write a text file for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracted the freq of each remaining tokens after stemming and removing stopwords \n",
    "#and selecting the top 100 unigrams and storing it\n",
    "unigram_dictCount={}\n",
    "for i,j in tokenised_dict3.items():\n",
    "    unigram_dictCount[i]=FreqDist((j)).most_common(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formating the unigrams according to the required output to be stored\n",
    "final_uni_file_str=str()\n",
    "for i,j in unigram_dictCount.items():\n",
    "    final_uni_file_str=final_uni_file_str+str(i)+':'+str(j)+'\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writting the 100uni.txt files \n",
    "unigramtxtfile = open(\"30823293_100uni.txt\", \"w\",encoding='UTF-8')\n",
    "unigramtxtfile.write(str(final_uni_file_str))\n",
    "unigramtxtfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 Generating top 200 Bigrams:\n",
    "\n",
    "- with the help of `nltk.collocations.BigramAssocMeasures()` and `nltk.collocations.BigramCollocationFinder.from_words` we will apply this to get the top 200 bigrams using pmi method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating top 200 bigrams from all tokens within data\n",
    "allbigram=[]\n",
    "bigram_measure=nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "#for i,j in unigaram_token.items():\n",
    "words_bi = list(chain.from_iterable(unigaram_token.values()))\n",
    "finder=nltk.collocations.BigramCollocationFinder.from_words(words_bi)\n",
    "allbigram.append(finder.nbest(bigram_measure.pmi,200))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 Creating Vocab of Words\n",
    "\n",
    "- now we have 200 bigrams and unigrams in we will format them to keep them in the list and create a formated list \n",
    "- we will iterate on the final list and sort it and then put it in a string that will make it formated as per the requirement \n",
    "- finally we will write the vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of bigrams with each bigram appened to each other as a word for vocab text\n",
    "bigramList=[]\n",
    "for i in allbigram:\n",
    "    for j in i:\n",
    "        if j not in bigramList:\n",
    "            bigramList.append(j[0]+'_'+j[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list of unigrams\n",
    "vocabList=[]\n",
    "for i,j in tokenised_dict3.items():\n",
    "    for x in j:\n",
    "        vocabList.append(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab = []\n",
    "finalVocab = []\n",
    "finalVocab=vocabList+bigramList\n",
    "Vocab=set(finalVocab)\n",
    "Vocab_list = list(Vocab)\n",
    "Vocab_list = sorted(Vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_vocab = ''\n",
    "for i in range(len(Vocab_list)):\n",
    "        string_vocab = string_vocab + str(Vocab_list[i]) + ':' + str(i) + '\\n'\n",
    "        \n",
    "\n",
    "# writting the vocab.txt files \n",
    "vocabtxtfile = open(\"30823293_vocab.txt\", \"w\",encoding='UTF-8')\n",
    "vocabtxtfile.write(str(string_vocab))\n",
    "vocabtxtfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writting the vocab.txt files \n",
    "vocabtxtfile = open(\"30823293_vocab.txt\", \"w\",encoding='UTF-8')\n",
    "vocabtxtfile.write(str(string_vocab))\n",
    "vocabtxtfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "In this task I have created tokens for each date. Then I have carried out steps which includes removal of stopwords, context independent words, context dependent words amd words with length less than 3. After this I have carried out stemming for each token. Hence I have created unigrams and vocab files. Also I have generated bigrams which occur the most frequently using collocations and pmi measure techniques. Hence the following files have been generated. \n",
    "\n",
    "Hence after this task I have gained knowledge on how to work with nltk library and I have understood the concepts of unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
